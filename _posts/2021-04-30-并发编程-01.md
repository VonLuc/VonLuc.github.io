---
layout:     post   				    							
title:      并发编程01 				
subtitle:   7天掌握并发编程
date:       2021-04-30 											
author:     Zhan 												
header-img: img/post-bg-2015.jpg 								
catalog: true 													
tags:														
    - 并发编程 
---

# 并发编程01

​	因公司项目需要，目前在快速阅读学习由方腾飞作，机械工业出版社出版的《java并发编程艺术》，希望提炼精简相关知识点和个人思考总结，希望在7天内完成一个版本，系列文章以并发编程(01-07)的结构发篇处理，如出现理解偏差，欢迎评论指正；

## 术语描述

阅读前提假设java、线程相关概念理论、代码已熟悉；



## 使用并发会遇到的问题、并发的底层原理与实现

### 使用并发会遇到的问题

#### 上下文切换

​	多线程的执行，实质为CPU为每个线程分配时间片执行各自任务，时间片的分配有指定的算法实现，时间片一般为几十毫秒ms；

​	时间片的切换过程：切换前保存上一个任务的状态，切换线程，切换回原任务时加载之前保存的状态。任务的保存到再加载的过程为一次上下文切换；

​	并发中线程存在线程创建和上下文切换开销，对于这种情况我们需要量化监控:

​		·使用Lmbench3测量上下文切换的时长；

​		·使用vmstat测量上下文切换次数；

##### 如何减少上下文切换

​	减少上下文切换的方法有无锁并发变成、CAS算法、使用最少线程和使用协程；

​	无锁并发编程：多线程竞争锁时，会引起上下文切换，故多线程时可想办法避免使用锁，如将数据的ID按照Hash算法取模分段，不同线程处理不同段的数据；

​	CAS算法：java的Atomic包使用CAS算法更新数据，且不需要加锁；

​	使用最少线程：避免创建不需要的线程，比如任务少缺创建很多线程会造成大量线程都处于等待状态；

​	协程：单线程里实现多任务调度，并在单线程里维持多个任务间的切换 ；

##### 减少上下文切换实战

###### 通过减少线上大量waiting线程，减少上下文切换次数

​	1.jstack查看线程pid进程里线程工作情况；

​		sudo -u admin /opt/ifeve/java/bin/jstack 31177 > /home/dump17

​	2.统计所有线程分别处于何状态，查看多少线程处于waiting(onobjectmonitor)状态；

```
grep java.lang.Thread.State dump17 | awk '{print $2$3$4$5}'
           | sort | uniq -c
    39 RUNNABLE
    21 TIMED_WAITING(onobjectmonitor)
    6 TIMED_WAITING(parking)
    51 TIMED_WAITING(sleeping)
    **305 WAITING(onobjectmonitor)**
    3 WAITING(parking)
```

​	3.进入dump文件查处于WAITING(onobjectmonitor)线程在处理什么，发现大多数为jboss工作线程在await，说明jboss线程池里线程收到任务少，大量线程都闲着；

```
"http-0.0.0.0-7001-97" daemon prio=10 tid=0x000000004f6a8000 nid=0x555e in
       Object.wait() [0x0000000052423000]
    java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00000007969b2280> (a org.apache.tomcat.util.net.AprEndpoint$Worker)
    at java.lang.Object.wait(Object.java:485)
    at org.apache.tomcat.util.net.AprEndpoint$Worker.await(AprEndpoint.java:1464)
    - locked <0x00000007969b2280> (a org.apache.tomcat.util.net.AprEndpoint$Worker)
    at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1489)
    at java.lang.Thread.run(Thread.java:662)
```

​	4.减少jboss工作线程，找到jboss的线程池配置，将maxThreads降到100；

```
<maxThreads="250" maxHttpHeaderSize="8192"
    emptySessionPath="false" minSpareThreads="40" maxSpareThreads="75"
        maxPostSize="512000" protocol="HTTP/1.1"
    enableLookups="false" redirectPort="8443" acceptCount="200" bufferSize="16384"
    connectionTimeout="15000" disableUploadTimeout="false" useBodyEncodingForURI= "true">
```

​	5.重启jboss，再dump线程信息，后统计WAITING(onojbectmonitor)线程，发现减少了175个。WAITING线程少了，系统上下文切换次数就会少，原因是每一次从WAITING到RUNNABLE都会进行一次上下文切换，可使用vmstat命令测试；

```
grep java.lang.Thread.State dump17 | awk '{print $2$3$4$5}'
       | sort | uniq -c
      44 RUNNABLE
      22 TIMED_WAITING(onobjectmonitor)
      9 TIMED_WAITING(parking)
      36 TIMED_WAITING(sleeping)
      **130 WAITING(onobjectmonitor)**
   1  WAITING(parking)
```

#### 死锁

​	避免死锁的常见方法：

​		·避免一个线程同时获取多个锁；

​		·避免一个线程在锁内同时占多个资源，尽量保证每个锁只占用一个资源；

​		·尝试使用定时锁，使用lock.tryLock(timeout)替代使用内部锁机制；

​		·对于数据库锁，加锁和解锁必须在一个数据库连接里，否则解锁失败；

#### 资源限制的挑战

	##### 资源限制

​	资源限制是进行并发编程时，程序的执行速度受限于计算机硬或软件资源；

​	硬件资源限制：

​		宽带的上传/下载速度、硬盘读写速度、cpu处理速度；

​	软件资源限制:

​		数据库连接数、socket连接数等；

##### 资源限制引发的问题

​	并发编程中将代码执行速度加快的原则为将代码中串行执行的部分变成并发执行，此时该部分处理会受限于资源，仍并行执行由于增加了上下文切换和资源调度的时间，会反而让程序变慢；

##### 如何解决资源限制问题

​	对于硬件资源限制，考虑用集群并行执行，如odps、hadoop搭建服务器集群，不同机器处理不同数据，以数据id%机器数，计算得到一个机器编号，后由对应编号的机器处理该笔数据；

​	对于软件资源限制，可考虑使用资源池将资源复用，如使用连接池将数据库和socket连接复用，或者在调用对方webservice接口获取数据时，只建立一个连接；

##### 在资源限制情况下进行并发编程

​	如何在资源限制情况下，让程序更快？方法为根据不同的资源限制调整程序的并发度，如下载文件依赖带宽、硬盘读写速度。有数据库操作时，涉及数据库连接数，如sql执行非常快，而线程的数量比数据库连接数大很多，则某些线程会被阻塞，等待数据库连接。



## java并发机制的底层实现原理

### volatile的应用

​	volatile是轻量级synchronized，在多处理器开发中保证共享变量的可见性；

​	可见性可以理解为当一个线程修改一个共享变量时，另一个线程能读到这个修改的值。若volatile恰当使用时，会比synchronized的使用和执行成本低，因其不会引起线程的上下文切换和调度。接下来将从硬件层面上intel处理器如何实现volatile入手深入分析，以便我们可以正常使用volatile变量；

#### volatile的定义与实现原理

​	java语言规范第三版中定义为：java允许线程访问共享变量，为确保共享变量能被准确和一致更新，线程应该确保通过***排他锁***__***单独***_获得这个变量；

​	在某些情况下，volatile比锁更加方便，一个字段被声明为volatile，java线程内存模型确保所有线程看到这个变量的值上一致的；

与volatile实现原理相关cpu术语说明:

内存屏障：memory barries，用于实现对内存操作的顺序限制的一组处理器指令；

缓冲行：cache line，缓存中可分配的最小存储单位。处理器填写缓存线时会加载整个缓存线，需要使用多个主内存读周期（关于缓存行部分，比较复杂且涉及的问题点较多，会另起一篇文章来描述）；

原子操作：atomic operations，不可中断的一个或一系列操作；

缓存行填充：cache fill fill，当处理器识别到从内存中读取操作数是可缓存的，处理器读取整个缓存行到适当的缓存（L1（一级缓存）,L2（二级缓存）,L3（三级缓存）的或所有)；

缓存命中:cache hit，当处理器将操作数写回到一个内存缓存的区域时，首先检查这个缓存的内存地址是否在缓存行中，若存在一个有效的缓存行，则处理器将此操作数写回到缓存，而不是写回到内存，这个操作叫写命中；

写缺失：write misses the cache，一个有效的缓存行被写入到不存在的内存区域；

#### volatile如何保证可见性

volatile instance = new Singleton();//instance为volatile变量

对应汇编为：

0x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp);

经查IA-32架构软件开发者手册，lock指令在多核处理器下作用：

1.将当前处理器缓存行的数据写回到系统内存；

2.此写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效；

  为提高处理速度，处理器不直接和内存通信，而是先将系统内存的数据读取内部缓存(L1,L2或其他)后再操作，操作完后不知何时会写到内存。若对声明了volatile的变量进行写操作，JVM向处理器发送一条lock前缀指令，将该变量所在缓存行的数据写回到系统内存。写会内存时如果其他处理器缓存的值还是旧的，在执行计算操作会有问题，因此多处理器下，为保证各个处理的缓存一致，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检测自己缓存的值是不是过期了，当处理器发现自己的缓存行对应的内存地址被修改，就将当前处理器的缓存行设置为无效状态，当处理器对这个数据进行修改操作时，会重新从系统内存中把数据读到处理器缓存里。

#### volatile的实现原则

​	1）lock前缀指令会引起处理器缓存回写到内存；

​			lock前缀指令导致在执行指令期间，声言处理器的lock信号，多处理器环境中，lock信号确保在声言该信号期间，处理器可以独占任何共享内存。但是在最近的处理器中，lock信号一般不锁总线，而是锁缓存(锁总线开销大)。对于Intel486和Pentium处理器,锁操作时总是在总线上声言lock信号。但P6和目前处理器中,若访问的内存区域已缓存在处理期内部,则不会声言lock信号。而是锁定这块内存区域的缓存并回写到内存，并使用缓存一致性极致确保修改的原子性，此操作为"缓存锁定"，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。

​	 2）一个处理器的缓存回写到内存会导致其他处理的缓存无效；















